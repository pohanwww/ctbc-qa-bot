# LoRA Fine-tuning Configuration for CTBC QA Bot
# ==============================================
#
# This configuration is optimized for:
# - Qwen3-4B base model
# - Single GPU with 16GB VRAM (e.g., T4)
# - QLoRA (4-bit quantization) for memory efficiency

# Model Settings
# --------------
model_id: "Qwen/Qwen3-4B"
hf_token: null  # Set via HF_TOKEN env var if needed
trust_remote_code: true

# LoRA Settings
# -------------
# r: Rank of the low-rank matrices (higher = more capacity, more VRAM)
# alpha: Scaling factor (usually 2x rank)
# dropout: Regularization
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# Quantization Settings (QLoRA)
# -----------------------------
use_4bit: true
bnb_4bit_compute_dtype: "float16"
bnb_4bit_quant_type: "nf4"
use_double_quant: true

# Training Settings
# -----------------
output_dir: "artifacts/models/lora_adapter"
num_train_epochs: 3
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 4  # Effective batch size = 4 * 4 = 16
learning_rate: 0.0002  # 2e-4
weight_decay: 0.01
warmup_ratio: 0.03
max_seq_length: 2048

# Data Paths
# ----------
train_data_path: "data/processed/finetune/train.jsonl"
val_data_path: "data/processed/finetune/val.jsonl"

# Logging & Checkpointing
# -----------------------
logging_steps: 10
save_steps: 100
eval_steps: 100

# Precision & Optimization
# ------------------------
seed: 42
fp16: true
bf16: false  # Set to true if using A100 or newer GPU
gradient_checkpointing: true

# Weights & Biases (Optional)
# ---------------------------
wandb_project: null  # Set to enable W&B logging
wandb_run_name: null

