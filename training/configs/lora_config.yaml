# LoRA Fine-tuning Configuration for CTBC QA Bot
# ==============================================
#
# This configuration is optimized for:
# - Qwen3-4B base model
# - Single GPU with 16GB+ VRAM (L4/A100)
# - QLoRA (4-bit quantization) for memory efficiency

# Model Settings
# --------------
model_id: "Qwen/Qwen3-4B"
hf_token: null  # Set via HF_TOKEN env var if needed
trust_remote_code: true

# LoRA Settings
# -------------
# r: Rank of the low-rank matrices (higher = more capacity, more VRAM)
# alpha: Scaling factor (usually 2x rank)
# dropout: Regularization
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# Quantization Settings (QLoRA)
# -----------------------------
use_4bit: true
bnb_4bit_compute_dtype: "bfloat16"  # Changed to bf16 for L4
bnb_4bit_quant_type: "nf4"
use_double_quant: true

# Training Settings
# -----------------
output_dir: "artifacts/models/lora_adapter"
num_train_epochs: 1  # Just 1 epoch for quick training
per_device_train_batch_size: 8  # Increased from 4
per_device_eval_batch_size: 8
gradient_accumulation_steps: 8  # Effective batch size = 8 * 8 = 64
learning_rate: 0.0001  # Increased slightly for faster convergence with larger batch
weight_decay: 0.01
warmup_ratio: 0.03
max_seq_length: 1024  # Reduced from 2048 for faster processing

# Data Paths
# ----------
train_data_path: "data/processed/finetune/train.jsonl"
val_data_path: "data/processed/finetune/val.jsonl"

# Logging & Checkpointing
# -----------------------
logging_steps: 20  # Log every 20 steps
save_steps: 200  # Save checkpoint every 200 steps
eval_steps: 200  # Eval every 200 steps

# Precision & Optimization
# ------------------------
seed: 42
fp16: false
bf16: true  # Enable bf16 for L4 GPU (better numerical stability)
gradient_checkpointing: true

# Weights & Biases (Optional)
# ---------------------------
wandb_project: null  # Set to enable W&B logging
wandb_run_name: null

